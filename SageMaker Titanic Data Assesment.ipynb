{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c87be99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import pandas as pd\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket and prefix\n",
    "bucket = sess.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-Titanic\"\n",
    "\n",
    "# Define IAM role\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e02a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import io\n",
    "import time\n",
    "import json\n",
    "import sagemaker.amazon.common as smac\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31259933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the test.csv and train.csv datasets using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f863eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain = pd.read_csv('train.csv')\n",
    "dftest = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1d9fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0498e1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8398d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b4a383",
   "metadata": {},
   "source": [
    "#### Question: Drop the unwanted columns ['PassengerId','Ticket','Cabin','Name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bedea60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06415bcc",
   "metadata": {},
   "source": [
    "#### Question: Encoding for the categorical variable 'Sex' with 'male' as 1 and 'female' as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77695a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edcf491f",
   "metadata": {},
   "source": [
    "#### Impute the missing age value with the mean of the age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f502fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.Age = dftrain['Age'].fillna(dftrain.Age.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e493d2",
   "metadata": {},
   "source": [
    "#### Question: Drop all the missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e000ef5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92e54990",
   "metadata": {},
   "source": [
    "#### Question: Encode the Embarked column with the following {'S':0, 'C':1, 'Q':2} values as integer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed58170d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4809a801",
   "metadata": {},
   "source": [
    "#### follow the similar data cleaning steps with the testing data 'dftest' as you did above with the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe6c32c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed108ce9",
   "metadata": {},
   "source": [
    "Note: All the changes to the training data frame should be saved to the variable 'dftrain' to execute the below code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a48bd4",
   "metadata": {},
   "source": [
    "## Create Features and Labels\n",
    "#### Split the data into 80% training, 20% validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc8fc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_split = np.random.rand(len(dftrain))\n",
    "train_list = rand_split < 0.8\n",
    "val_list = rand_split >= 0.8\n",
    "\n",
    "\n",
    "data_train = dftrain[train_list]\n",
    "data_val = dftrain[val_list]\n",
    "\n",
    "\n",
    "train_y = data_train.iloc[:, 0].to_numpy()\n",
    "train_X = data_train.iloc[:, 1:].to_numpy()\n",
    "\n",
    "val_y = data_val.iloc[:, 0].to_numpy()\n",
    "val_X = data_val.iloc[:, 1:].to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f973e95",
   "metadata": {},
   "source": [
    "Now, we'll convert the datasets to the recordIO-wrapped protobuf format used by the Amazon SageMaker algorithms, and then upload this data to S3.  We'll start with training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ba1f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"linear_train.data\"\n",
    "\n",
    "f = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(f, train_X.astype(\"float32\"), train_y.astype(\"float32\"))\n",
    "f.seek(0)\n",
    "\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(prefix, \"train\", train_file)\n",
    ").upload_fileobj(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2dff49",
   "metadata": {},
   "source": [
    "Next we'll convert and upload the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e13f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_file = \"linear_validation.data\"\n",
    "\n",
    "f = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(f, val_X.astype(\"float32\"), val_y.astype(\"float32\"))\n",
    "f.seek(0)\n",
    "\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(prefix, \"validation\", validation_file)\n",
    ").upload_fileobj(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329f06e1",
   "metadata": {},
   "source": [
    "## Training the linear model\n",
    "\n",
    "Once we have the data preprocessed and available in the correct format for training, the next step is to actually train the model using the data. Since this data is relatively small, it isn't meant to show off the performance of the Linear Learner training algorithm, although we have tested it on multi-terabyte datasets.\n",
    "\n",
    "Again, we'll use the Amazon SageMaker Python SDK to kick off training, and monitor status until it is completed.  In this example that takes between 7 and 11 minutes.  Despite the dataset being small, provisioning hardware and loading the algorithm container take time upfront.\n",
    "Now we can begin to specify our linear model.  Amazon SageMaker's Linear Learner actually fits many models in parallel, each with slightly different hyperparameters, and then returns the one with the best fit.  This functionality is automatically enabled.  We can influence this using parameters like:\n",
    "\n",
    "- `num_models` to increase to total number of models run.  The specified parameters will always be one of those models, but the algorithm also chooses models with nearby parameter values in order to find a solution nearby that may be more optimal.  In this case, we're going to use the max of 32.\n",
    "- `loss` which controls how we penalize mistakes in our model estimates.  For this case, let's use absolute loss as we haven't spent much time cleaning the data, and absolute loss will be less sensitive to outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f67a70",
   "metadata": {},
   "source": [
    "### Specify container images used for training and hosting SageMaker's linear-learner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434cf011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris\n",
    "\n",
    "container = image_uris.retrieve(region=boto3.Session().region_name, framework=\"linear-learner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632628dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_job = \"DEMO-linear-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "print(\"Job name is:\", linear_job)\n",
    "\n",
    "linear_training_params = {\n",
    "    \"RoleArn\": role,\n",
    "    \"TrainingJobName\": linear_job,\n",
    "    \"AlgorithmSpecification\": {\"TrainingImage\": container, \"TrainingInputMode\": \"File\"},\n",
    "    \"ResourceConfig\": {\"InstanceCount\": 1, \"InstanceType\": \"ml.c4.2xlarge\", \"VolumeSizeInGB\": 10},\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": \"s3://{}/{}/train/\".format(bucket, prefix),\n",
    "                    \"S3DataDistributionType\": \"ShardedByS3Key\",\n",
    "                }\n",
    "            },\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"RecordWrapperType\": \"None\",\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": \"s3://{}/{}/validation/\".format(bucket, prefix),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                }\n",
    "            },\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"RecordWrapperType\": \"None\",\n",
    "        },\n",
    "    ],\n",
    "    \"OutputDataConfig\": {\"S3OutputPath\": \"s3://{}/{}/\".format(bucket, prefix)},\n",
    "    \"HyperParameters\": {\n",
    "        \"feature_dim\": \"7\",\n",
    "        \"mini_batch_size\": \"100\",\n",
    "        \"predictor_type\": \"binary_classifier\",\n",
    "        \"epochs\": \"10\",\n",
    "        \"num_models\": \"32\",\n",
    "        \"loss\": \"absolute_loss\",\n",
    "    },\n",
    "    \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 60 * 60},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef4f82b",
   "metadata": {},
   "source": [
    "Now let's kick off our training job in SageMaker's distributed, managed training, using the parameters we just created.  Because training is managed, we don't have to wait for our job to finish to continue, but for this case, let's use boto3's 'training_job_completed_or_stopped' waiter so we can ensure that the job has been started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515de07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "sm.create_training_job(**linear_training_params)\n",
    "\n",
    "status = sm.describe_training_job(TrainingJobName=linear_job)[\"TrainingJobStatus\"]\n",
    "print(status)\n",
    "sm.get_waiter(\"training_job_completed_or_stopped\").wait(TrainingJobName=linear_job)\n",
    "if status == \"Failed\":\n",
    "    message = sm.describe_training_job(TrainingJobName=linear_job)[\"FailureReason\"]\n",
    "    print(\"Training failed with the following error: {}\".format(message))\n",
    "    raise Exception(\"Training job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecfbb2e",
   "metadata": {},
   "source": [
    "Once we've setup a model, we can configure what our hosting endpoints should be.  Here we specify:\n",
    "1. EC2 instance type to use for hosting\n",
    "1. Initial number of instances\n",
    "1. Our hosting model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2383033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_endpoint_config = \"DEMO-linear-endpoint-config-\" + time.strftime(\n",
    "    \"%Y-%m-%d-%H-%M-%S\", time.gmtime()\n",
    ")\n",
    "print(linear_endpoint_config)\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=linear_endpoint_config,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.m4.xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": linear_job,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd904de0",
   "metadata": {},
   "source": [
    "Now that we've specified how our endpoint should be configured, we can create them.  This can be done in the background, but for now let's run a loop that updates us on the status of the endpoints so that we know when they are ready for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e11955",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "linear_endpoint = \"DEMO-linear-endpoint-\" + time.strftime(\"%Y%m%d%H%M\", time.gmtime())\n",
    "print(linear_endpoint)\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=linear_endpoint, EndpointConfigName=linear_endpoint_config\n",
    ")\n",
    "print(create_endpoint_response[\"EndpointArn\"])\n",
    "\n",
    "resp = sm.describe_endpoint(EndpointName=linear_endpoint)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "sm.get_waiter(\"endpoint_in_service\").wait(EndpointName=linear_endpoint)\n",
    "\n",
    "resp = sm.describe_endpoint(EndpointName=linear_endpoint)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "if status != \"InService\":\n",
    "    raise Exception(\"Endpoint creation did not succeed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdb0b3b",
   "metadata": {},
   "source": [
    "## Predict\n",
    "### Predict on Test Data\n",
    "\n",
    "Now that we have our hosted endpoint, we can generate statistical predictions from it.  Let's predict on our test dataset to understand how accurate our model is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de098c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invoking the endpoint for predictions for a sample record with pay load as input record\n",
    "\n",
    "runtime = boto3.client(\"runtime.sagemaker\")\n",
    "\n",
    "payload = \"1,0,34.5,0,0,7.8292,3\"\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=linear_endpoint, ContentType=\"text/csv\", Body=payload\n",
    ")\n",
    "result = json.loads(response[\"Body\"].read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba1fa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03eec8e",
   "metadata": {},
   "source": [
    "#### Question: Inkove on the sample record provided in the assessment payload = \"1,1,34.5,0,0,7.8292,2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843eac3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac06dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c42cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To predict on the entire test data, clean the data by removing the columns and preparing the data as similar to training data set and converting the data to required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1819ff91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f894ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np2csv(arr):\n",
    "    csv = io.BytesIO()\n",
    "    np.savetxt(csv, arr, delimiter=\",\", fmt=\"%g\")\n",
    "    return csv.getvalue().decode().rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd742efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = boto3.client(\"runtime.sagemaker\")\n",
    "test_X = dftest.to_numpy();\n",
    "payload = np2csv(test_X)\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=linear_endpoint, ContentType=\"text/csv\", Body=payload\n",
    ")\n",
    "result = json.loads(response[\"Body\"].read().decode())\n",
    "test_pred = np.array([r[\"predicted_label\"] for r in result[\"predictions\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb7a95e",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98863df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting the endpoint\n",
    "sm.delete_endpoint(EndpointName=linear_endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
